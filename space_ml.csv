id_c0nv,id_old,x,y,n_tweets,tree_size,n_orphans,tree_depth,tree_width,conv_start,conv_end,Justification,Topics,disagreement
0,'1002437369356791808',-49.86453552,0.205208405,80,48,32,13,17,21/01/1900 09:37,06/01/2018 06:31,it's about the importance of symbolic manipulation; Marcus agreed that hybrid models are a way to go;,hybrid models;,2
1,'1065280340669816832',-48.85597493,0.134792465,361,241,120,41,83,01/04/1900 13:46,11/21/2018 16:26,Marcus reacts to Bengio's opinion on the future of DL (and its current limits); LeCun reponds; discussion develops into who has done what to (scientifically) substantiate/communicate their arguments?,"limits of DL, scientific communication",2
2,'1066062398421819392',-47.84991708,0.206638815,92,25,67,8,12,14/01/1900 10:07,11/23/2018 20:14,"Marcus on the need to integrate symbol manipulation with learning from big data; LeCun replies in agreement but notes that he believes DL can support learning from data, an idea that he thinks Marcus disagrees with;",hybrid models for AI,1
3,'1066274254285496320',-46.84024414,0.115285337,187,172,15,43,49,05/03/1900 04:36,11/24/2018 10:16,"about national investments in the AI research, specifically DL; is DL a good baseline for AI ""success""; role of GOFAI; DL for AGI; LeCun tells that it's all DL at Google and Meta",(national) AI research funding;DL as AI research baseline;AGI,2
4,'1070559287662034945',-45.83466378,0.150108761,108,106,2,32,32,14/02/1900 06:06,12/06/2018 06:03,A thread reacting to Marcus piece; Blake Richards puts out an argument that DL is a research program (links to LeCun and Bengio's 2007 paper),definition of DL;DL as research program,1
5,'1070736319901519876',-44.82798943,0.20878443,101,52,49,5,34,03/02/1900 08:46,12/06/2018 17:46,"Marcus tweets his ""The Next Decade in AI"" paper;",hybrid models for AI;brain,1
6,'1071438612690960384',-43.81834663,-0.856093629,377,275,102,36,112,26/04/1900 15:26,12/08/2018 16:17,Marcus' open letter to LeCun (and he responds) in the context of achieving AGI,AGI,2
7,'1073998486033612800',-42.8108018,0.043758022,109,86,23,18,34,07/02/1900 11:17,12/15/2018 17:49,this is a reaction to Wired's interview with Hinton; the discussion is around NN explainability and to what extend understanding is needed for trust,DL explainability,2
8,'1084859265121083392',-41.80320234,-0.797022744,96,75,21,12,43,13/02/1900 15:25,1/14/2019 17:06,"it's about what are the roadblocks to achieve AGI; Marcus makes a claim (?, some tweets seem to be deleted) on reasoning; there are disagreements on what are the main problems to overcome and if AGI is possile at all",AGI,2
9,'1126776530539958273',-40.79629216,0.0446876,117,106,11,18,40,12/02/1900 20:43,05/10/2019 09:10,it's a tweet on the predictive powers of ML when it has enough data,ML prediction,1
10,'1136501685302415360',-39.79034298,0.0594271,252,103,149,6,76,16/03/1900 05:40,06/06/2019 05:14,"This is a thread about AI replacing human basic coding jobs; there are a variety of opinions, but a number of them suggest to ""learn to code""",AI for code;task automation,2
11,'1173377561897816065',-38.78288274,0.141944515,207,166,41,15,75,16/03/1900 11:38,9/15/2019 23:26,Marcus on the current ML limitations and ML community's willingness to acknowledge them (LeCun suggests him listening to his past talks‚Ä¶),"limits of DL, communication (of those limits)",2
12,'1184299965390053377',-37.7768973,0.082429356,98,19,79,4,11,11/01/1900 16:54,10/16/2019 2:48,Marcus on OpenAI solving Rubik's Cube with a robot hand noting that it's a hype; there's some engagement with what OpenAi did but nobody really arguing that it‚Äôs not the hype,OpenAI;robotics,0
13,'1185679169360809984',-36.7703437,0.103291374,276,200,76,10,98,07/04/1900 12:12,10/19/2019 22:08,Marcus accuses OpenAI for a misleading blog title; OpenAI co-founder @woj_zaremba replies,"OpenAI, robotics, reinforcement learning",2
14,'1186808779431645184',-35.76303449,0.104006579,82,75,7,32,16,04/02/1900 18:39,10/23/2019 0:57,Marcus on DRL and Rubik's Cube (I reckon it's a reference to OpenAI's robot hand); Marcus argues that the solutions so far have not been pure DRL but rather hybrids; the discussion is about solving Rubik's Cube using RL,OpenAI;robotics,2
15,'1187596943830048768',-34.75092557,-0.039818142,109,90,19,22,37,12/02/1900 01:06,10/25/2019 5:09,"Marcus on current DL systems not understanding language; there is a discussion on the definition of ""understanding"" and its metrics",DL and semantics,2
16,'1193097279093256197',-33.74568375,0.111687824,77,74,3,22,26,03/02/1900 01:24,11/09/2019 09:25,it's on the LeCun and Marcus debate in NYU;,Symbolic AI vs DL;,1
17,'1197241701175160833',-32.73917845,0.064433535,214,203,11,5,140,19/05/1900 02:08,11/20/2019 19:53,"In this thread @Grady_Booch irodinally reacts to the comment that AI will take over coding jobs; in the thread users agree that it is extremely unlikely that I will happen (take away jobs), especially on the more abstract programming level",AI for code;task automation,1
18,'1197700438281379841',-31.73057568,0.045523424,148,57,91,30,17,03/02/1900 11:33,11/22/2019 2:16,Marcus tweets about the role of DL in physics; generally people tell him he has no competence in the field and should stick to human cognition and AI,DL and physics;,2
19,'1201233472989356032',-30.72566037,0.2187973,108,88,20,6,53,22/02/1900 08:07,12/01/2019 20:15,LeCun accuses GM for creating and promoting a false dychotomy between connectionsim and symbolic ai and that he should show how hybrid models would/could be merged; some note that connectionism is hybrid;,hybrid models for AI;,2
20,'1201260764339683328',-29.71534665,0.432870779,95,16,79,4,9,09/01/1900 20:22,12/01/2019 22:04,"Marcus' response to LeCun comment on creating false ""wars"" and also link to Hinton's talk on symbolic AI vs connectionism; LeCun makes a (bold?) claim that Marcus' entire career is built around being anti-connectionist",hybrid models for AI;AGI;politics of AI,2
21,'1201264073129824259',-28.71191235,0.290887596,55,45,10,11,25,27/01/1900 07:30,12/01/2019 22:17,a reaction to Hinton's differentiation between symbolic and connectionst AI; the argument goes that just because DL isn't perfect just yet it doesn't mean it has to be combined with symbolic approach; some challenge this by pointing out to the ontology of symbols,Symbolic AI vs DL;hybrid models for AI,2
22,'1202366200811884545',-27.69941049,0.243583562,82,77,5,8,40,09/02/1900 19:00,12/04/2019 23:16,"Marcus on an interview with Meta'a head of AI on field hitting the wall; noted that it was mentioned in reference to a specific technical point; Marcus seems to approve the Head of AI's observations, though other claim the claim to be unjustified; LeCun replies that the future of learning is self-supervised",DL hitting a wall;,2
23,'1209266195678973952',-26.69508739,0.097570803,108,93,15,12,31,02/02/1900 05:47,12/24/2019 0:15,original tweet on DL being rebranded to which Marcus reacted ('1209640096900812800') and commented in response to @tdietterich that having a broad definition of DL will make it hard to make claims in relation to DL,definition of DL;DL as research program,1
24,'1209283302089097218',-25.68679895,-0.010013197,129,92,37,16,38,10/02/1900 05:32,12/24/2019 1:23,"Mitchell reacts to Bengoi vs Marcus debate by noting that she doesn't believe in the strict system 1 and system 2 separation; Marcus replies in agreement and notes time limits to go into this; some disagree referencing Kahneman's work (interestingly, one linked to Retraction Watch post in which Kahneman's agrees he made some mistakes in the book); others note that having this distinction is a good metaphor;",Symbolic AI vs DL;brain,2
25,'1209640096900812800',-24.68046895,0.099001213,194,164,30,14,75,16/03/1900 07:05,12/25/2019 1:00,"Marcus reacts to @Raamana_ tweet who points the paper that ""rebrands"" DL research. Marcus addresses DL community by asking 4 questions, one of them is how deep learning should/could be defined",definition of DL,2
26,'1210280303173881856',-23.67449584,0.223803735,59,28,31,10,8,12/01/1900 19:21,12/26/2019 19:24,"this is a thread on symbols an connectionism in which @tdietterich note that connectionists need symbols, but the question remains how to implment them in a way that is compatible with connectionism; LeCun jumps in to agree and note that symbols need to be replaced by vectors (this is (becoming) a point of contention)",neuro-symbolic AI;symbol implementation in DL;DL research,1
27,'1210552584680796161',-22.66475029,0.418753758,88,81,7,18,25,30/01/1900 19:20,12/27/2019 13:26,Judea Pearl reacts to Marcus on the aims of DL and using it as a criteria for evaluating its success; there is some disagreement on what DL does (reference to LeCun as well),definition of DL;AI politics,2
28,'1211315213271568384',-21.6598774,0.225234145,155,149,6,21,51,24/02/1900 03:42,12/29/2019 15:57,"Marcus on hybrid models for AI (they are necessary, but not sufficient); discussion of DL for AGI anddoes DL imitate brain funcitoning?",hybrid models for AI;brain,1
29,'1211690022992318464',-20.64869326,0.368647738,100,97,3,23,46,20/02/1900 10:18,12/30/2019 16:46,Marcus reacts to the idea that it's not to talk about AI rather than DL by proposing that today to talk about AI is to talk about DL; Marcus asks to provide examples of AI that are not DL; users suggest such instances,DL as AI,1
30,'1213288484372459520',-19.64279856,0.054821661,82,4,78,2,2,02/01/1900 19:52,01/04/2020 02:38,It's about different ontological understanding of intelligence embeeded in symbolic AI and Connectionism;,definition of intelligence,2
31,'1221185173091426305',-18.63552554,-0.822514128,252,45,207,13,17,21/01/1900 09:37,1/25/2020 21:36,"a critical (and sarcastic?) take on Marcus' views; an root tweet author claims that Marcus will claim that DL is useless when GPT-9 writes novels; Marcus replies that he doesn't argue that even current GPT-2 cannot write one, but rather that it still needs human intervention; a discussion also tocuhes onto AGI (ie GPT-2 is a step forward)",GPT-2;AGI,2
32,'1229216106377646080',-17.63128735,0.199592347,47,43,4,7,27,27/01/1900 21:25,2/17/2020 1:28,"Marcus tweets about publishing The Next Decade in AI; reactions are mostly ""oh it's so interesting"" or ""it's a great read""; however @amolk critically engages and challenges some of the points made in the paper, including Marcus take on GPT-2","GPT-2, Symbolic AI",1
33,'1237140081208631297',-16.6220799,0.077941426,106,2,104,1,1,01/01/1900 09:56,03/09/2020 22:15,in this thread @eripsa reacts to @Abebab comment on a robot that self-taught; the discussion revolves around the degree of human engagement in that self-learning process; the argument goes that DL requires less (but not none!) engagement than Symbolic AI,DL in robotics;,2
34,'1244111977632468993',-15.61425686,0.098928176,117,25,92,8,11,13/01/1900 14:26,3/29/2020 3:59,"It's a thread in which @zacharylipton speculates that ML/DL is in the spotlight is due to its open-acess publishing culture, which is different from, e.g. philosophy; LeCun responds in agreement noting that this is not an accident and notes Andrew Gelman's paper",scientific communication;open-access,1
35,'1264839572351078401',-14.60602872,-0.833855391,1547,14,1533,6,6,08/01/1900 11:38,5/25/2020 8:43,Discusses DeepMind's approach to AGI and the im/possibility of AGI,AGI,2
36,'1267215588214136833',-13.59741386,-0.997719363,213,3,210,2,1,02/01/1900 05:39,5/31/2020 22:10,it's about if scaling up is all it will take to reach AGI; this is discussed in the context of GPT-2/3/4/n,Scaling DL;AGI;GPT-3,2
37,'1267457924965830658',-12.59169437,-0.833044574,302,23,279,11,9,14/01/1900 05:06,06/01/2020 14:08,the conversation is about AGI and (degree of current) understanding of brain structure and functioning,"AGI, brain",2
38,'1284843403201540096',-11.5840648,-0.831554884,111,105,6,8,60,29/02/1900 12:44,7/19/2020 13:31,this is a thread about AI learning or evolving to process symbols and perform causal inference; some propose that it might be useful to hand-craft some of those emergent properties,"AGI, symbol processing",2
39,'1284922296348454913',-10.57610277,-0.045765168,271,202,69,9,154,02/06/1900 06:18,7/19/2020 18:45,the thread is on GPT-3 and its role in reaching AI. There seems to be a consensus that it's exciting but still far from AI; it also discusses GPT-3 in the context of Turing Test (examples: would GPT-3 passs it? Wasn't Turing test already been passed?),"GPT-3, Turing test",2
40,'1284994545327972352',-9.569083807,0.076180897,101,101,0,11,53,23/02/1900 03:06,7/19/2020 23:32,"this is a thread on the idea that some ML techniques feel right because it resembles human learning; for example, hierarchical RL is one such idea; there's some engagement with each idea",biological/human vs machine learning,1
41,'1285104499531698176',-8.561671711,-0.037472553,143,98,45,16,40,12/02/1900 01:57,7/20/2020 6:49,"this thread is about GPT-3 and the idea that the ""big story"" is not that it's smart but that a dumb thing can do things that once were considered as requiring intelligence. @Grady_Booch note that it's not not a novel observation and mentions Minsky‚Äôs Society of Mind; others question the definition of ""smart""",GPT-3;intelligence,2
42,'1286650379740934145',-7.554954971,0.068901493,238,65,173,7,36,05/02/1900 16:10,7/24/2020 13:12,This is a long thread by @alive_eth (with some comments from other users) on AI centralisation in the hands of the big tech,"AI de/centralisation, big tech",0
43,'1289278123100368896',-6.547053274,-0.036042143,102,16,86,4,6,07/01/1900 05:03,7/31/2020 19:13,"it's a basically single author thread on intelligence and GPT-3 being an ""amortised inference"" ; Marcus replies about the lack of inference reliability and also links to his NY article",GPT-3;intelligence,0
44,'1294752456170037249',-5.538021277,-0.144432417,87,74,13,7,36,05/02/1900 16:10,8/15/2020 21:46,"It's a thread on GPT-3 and understanding; Marcus replies that GPT-3 does not really understand because it doesn't integrate new information to derive causal inferences; there's a discussion on what ""understanding"" means",GPT-3;semantics,0
45,'1296678240271171585',-4.532537715,0.079756922,1048,693,355,28,264,21/09/1900 11:32,8/21/2020 5:19,discusses human vs machine learning; LeCun (to whom Marcus replies in agreement); representative from NeurIPS takes a change to promote its workshop?,biological/human vs machine learning,2
46,'1309116709710766081',-3.525452033,-0.769844955,896,240,656,31,83,28/03/1900 14:24,9/24/2020 13:05,"implementation of AGI (incl testing resources, hiring)",AGI,2
47,'1331614120412471297',-2.516008888,-0.974615844,147,134,13,13,68,09/03/1900 05:33,11/25/2020 15:02,"Marcus claims that GPT-3 is not an important step towards AGI because it doesn't represent abstract concepts or map from sematics to syntax; some note in agreement that simply scaling won't lead AGI, but others observe that it's not a not important step; others argue that AGI is an impossibility + questions of what is an AGI;",GPT-3;AGI;semantics,2
48,'1349398199937675264',-1.510936461,0.068528497,203,69,134,22,26,03/02/1900 01:24,1/13/2021 16:49,it discusses the potential of knowledge graphs in AI and computational semantics,knowledge graphs,2
49,'1373112777519230977',-0.507085055,0.306380302,79,78,1,6,59,28/02/1900 07:18,3/20/2021 3:22,"in this thread @fchollet argues that DL still struggles being reliable and stable in production, yet it excells in demos; some reply that it only needs to be better than a human or that perhaps this will always be a case for DL and calls for neurosymbolic AI",DL in production;hybrid models for AI,1
50,'1384173521245376520',0.503706153,0.070062168,142,117,25,14,70,11/03/1900 09:16,4/19/2021 15:54,"Marcus reacts to Domingos comment that algorithms will perpetuate bias as they are easier to change; Marcus disagrees, cite Timnit, etc.; there's a discussion on value implementation",algorithmic bias;,2
51,'1387419527462354945',1.511432481,0.114472168,231,127,104,18,37,10/02/1900 03:30,4/28/2021 14:52,"This is a thread on ML robustness, generalisability, and criteria of ""success"" which relies on large amount of training data for DL, yet there's a lack of generalisability; an argument goes that humans are able to generalise from small examples while machines cannot (there seems to be a dispute over the accuracy of this statement)","robustness, generalisability, success criteria",2
52,'1397012867023441927',2.519660561,0.421091075,315,97,218,12,40,10/02/1900 18:16,5/25/2021 2:13,"it's on history of AI, the changing notion of what is AI, and the similarity between Symbolic AI and Lisp","Lisp, POPL for AI;AI (winter) politics",0
53,'1420754021858512910',3.522895341,0.260664732,57,50,7,20,11,22/01/1900 19:48,7/29/2021 14:32,It's a reaction to Amazon Science tweeting about the potential need to implement symbol manipulation to achieve more complex machine reasoning; some express scepticism and winder if neuro-symbolic approach is intermediate/temporary solution until we get better data/models because symbols are hard to scale,hybrid models for AI;reasoning;Amazon,2
54,'1424498054393180167',4.533662427,-0.031955741,202,96,106,26,28,07/02/1900 05:02,08/08/2021 22:29,It's a thread of GPT-3,GPT-3,0
55,'1425968260483227651',5.539508746,0.089283241,127,31,96,14,7,15/01/1900 15:39,08/12/2021 23:51,It's about OpenAI Codex and GPT-3; the discussion seems to be mostly about the potential of using NL to generate code;,"OpenAI Codex, GPT-3;AI for code",1
56,'1457539962606288898',6.547633996,0.074663183,87,81,6,11,43,13/02/1900 09:13,11/08/2021 02:46,"Judea Pearl on the importance of having a causal model for ML to achieve explainability, trustworthiness, etc; some argue that progress was made without causality",ML causality,1
57,'1468689810671824896',7.555299836,0.082467617,78,67,11,7,48,17/02/1900 12:11,12/08/2021 21:11,"this thread is about the importance of GPU in DL advances; a couple of users refer to the ""hardware lottery"" pre-print",computing power;DL,1
58,'1472587433120313345',8.56185956,0.074847401,312,274,38,8,172,20/06/1900 04:27,12/19/2021 15:19,"LeCun's post on the need to change the debate surrounding intelligence in the AI. Move from language and symbols to ""world models"" (based on the idea that AI hasn't even achieved dog-level intelligence who don't manipulate symbols (Marcus questions this))",definition of intelligence,2
59,'1491493722583879681',9.568727449,0.091928566,148,147,1,8,111,20/04/1900 06:54,02/09/2022 19:26,"LeCun questions the use of RL for optimal control and planning problems and why people call something RL when it's not; there's a bunch of answers, including RL being trendy, RL not being that different from optimal planning, not knowing what control theory is",reinforcement learning,2
60,'1498761178293714947',10.57792265,-0.148225401,107,101,6,7,43,12/02/1900 13:35,03/01/2022 20:44,"in this thread @srush_nlp ""bets"" that transformer-like architectures will not benchmark all NLP tasks before 2027; @srush_nlp basically argues against scaling of DL",scaling DL;LLM,1
61,'1501734523628707845',11.58564903,0.271476557,171,138,33,6,86,26/03/1900 05:01,03/10/2022 01:39,"Marcus on DL limitations to advance AI (i.e. DL hitting the wall), AGI, AI winter?","AGI, DL (AI?) winter",2
62,'1502161663490813959',12.59405848,0.590513897,184,134,50,17,65,07/03/1900 04:28,03/11/2022 05:57,It's about Marcus' claim of DL hitting a wall; AGI; politics in AI research (e.g. investing a lot into DL while underfunding symbolic),DL hitting a wall;AI funding and politics,2
63,'1502337713315274760',13.60026747,0.272906967,119,80,39,13,50,20/02/1900 15:53,03/11/2022 17:36,in this thread @Plinz uses a meme to critque Marcus claim that DL is hitting a wall; some engage with meme's inaccuracy and/or note that DL implementation in real life is still problematic,DL hitting a wall;,2
64,'1502350612293296130',14.60571486,0.084023875,80,37,43,5,27,27/01/1900 11:01,03/11/2022 18:27,Marcus on the predictive power of large models (80% on addition problem?); some note that it's not bad given that humans are likely to struggle too;,prediction,1
65,'1502687227037589507',15.6146744,0.299531128,69,4,65,2,2,02/01/1900 19:52,03/12/2022 16:45,Here @AndrewLampinen (from DeepMind) reacts to Marcus claim that DL has hit the wall; Andrew suggests a reinterpretation of a symbol following Pierce; Marcus responds in reference to model performance evaluation,DL hitting a wall;Symbolic AI;model performance evaluation,1
66,'1503220020175769602',16.62084707,0.08233154,738,494,244,90,95,09/05/1900 20:42,3/14/2022 4:02,AI alignment bingo in the form of claims around AI and replies to them; AI aliginment; AGI,"AGI, AI alignment",1
67,'1503513921650634759',17.62819861,0.113497015,55,24,31,11,11,15/01/1900 13:21,3/14/2022 23:30,Mitchell reacts to DL vs Symbolic AI debate by sharing Newell's (1982) paper and noting that similar debates have been lasting for decades,Symbolic AI vs DL history,0
68,'1503719133506158596',18.63636627,0.396540732,59,58,1,7,36,05/02/1900 16:10,3/15/2022 13:05,LeCun reacts to Mitchell's thread by arguing that he's neverr claimed to be working on AI but rather that media conflated DL with AI at around 2013. Some wonder why it's an incorrect terminology and note that neural networks have always been subbranch of AI,AI politics,2
69,'1503792844330487820',19.64250875,0.214015639,360,288,72,35,110,24/04/1900 10:24,3/15/2022 17:58,it's about DL is the future because they workd better than symbolic models for AI; also discuss that those who believe in symbolic models should work harder and show that symbolid models work,DL works better than symbolic models;belief and investment in DL,2
70,'1504457604780765186',20.65167984,0.297976019,261,196,65,17,97,07/04/1900 11:28,3/17/2022 14:00,"LeCun's (indirect) response to Marcus' claim about AI hitting a wall, interesting disucssion on who is capable of criticising current AI research trends (e.g. if you criticise an idea then you need to back it up with practical results)","AI hitting a wall, expertise and criticism",2
71,'1505270032397336576',21.65832405,-0.140358146,110,85,25,31,30,12/02/1900 03:20,3/19/2022 19:48,"Marcus on the idea that scaling models up (more data, more computing) will not bring us closer to AI and/or AGI",Scaling DL,1
72,'1506707421074403331',22.66587515,0.428234421,53,44,9,11,18,00/01/1900 00:00,23/03/2022 19:00,"Marcus reacts to the claim of DL being more modular than classic ML models by arguing that symbolic models have been modular before NNs. Some users note that the focus was on DL vs ML, not symbolic models, yet Marcus responds that in the 1990s ML people were against modularity;",AI politics;modularity,2
73,'1511019587277664256',23.67285798,-0.892416962,258,206,52,30,114,26/04/1900 21:09,04/04/2022 16:35,Google introduces Pathways Language Model; majority of people seem to express excitement and mention getting closer to AGI,Google;PaLM;AGI,1
74,'1511733842876448772',24.68068694,0.000893968,119,112,7,32,19,06/02/1900 05:10,04/06/2022 15:53,"it's about DALL-E 2; there is a conversation of deliberate bias implemented into the model (e.g. excluding what is considered bad) and on the difference between drawing something that looks, like, e.g. a rabit, and actually drawing a rabit",DALL-E 2;ethics,2
75,'1512304474542546947',25.68622506,0.092391962,100,40,60,14,12,18/01/1900 10:32,04/08/2022 05:40,"in this thread @vectorpark argues that we know how ML works, thus claims about it being opaque are exaggerated, however we don't know how human brain works; he's being challenged that we know how ANNs work; Marcus replies, but the context is unlear to me",ML explainability,2
76,'1512455146248224770',26.69575885,-0.972007587,114,65,49,4,45,14/02/1900 04:15,04/08/2022 15:39,"Mitchell reacts to Dall-e 2 and Google's PaLM; she notes that while they are impressive, they are not AGI and there's still a long way to go; she discusses Bongard's test to exemplify the struggles current AI systems face",DALL-E 2;PaLM;AGI,1
77,'1513557173095129088',27.70286856,0.528782876,56,3,53,2,1,02/01/1900 05:39,04/11/2022 16:38,"Marcus on AI politics and the role of funding and how science is done; some implicitly note that DL works, thus it gets funding",AI politics;scientific communication;,1
78,'1514377540948807684',28.70879343,0.097486921,98,92,6,6,68,08/03/1900 06:20,4/13/2022 22:58,Domingos tweets that 75% of Google's computing power goes to ML,computing power,0
79,'1514619721341145091',29.71901025,-0.242857556,117,70,47,20,36,10/02/1900 04:22,4/14/2022 15:00,in this thread @raphaelmilliere expresses his position re Marcus claim that AI is hitting a wall in the context of DALL-E 2 and PaLM,DALL-E 2;PaLM;semantics,1
80,'1517588701743697921',30.72020196,0.262424804,190,143,47,11,89,29/03/1900 16:15,4/22/2022 19:38,"Domingo critiques the focus on DL in AI; constantly refers to ""Master Algorithm""",DL is not enough for AI,2
81,'1521638361353625600',31.73153725,0.481631341,87,80,7,12,45,15/02/1900 13:44,05/03/2022 23:50,it's a critical take on the idea that AI is successful by pointing out disproportional funding; some argue that funding came after it demonstrated good results (LeCun replies in agreement); Marcus point current funding assymetry which makes competition hard;,AI politics;research funding,2
82,'1522569051653951488',32.73838087,0.101961534,142,61,81,15,18,23/01/1900 10:20,05/06/2022 13:28,in this thread @AndrewLampinen reacts to a paper published by Microsoft researchers on the progress of AI in the last decade which they also attribute to neurocompositional computing; Andrew critiques the operationalisation of compositionality; one of the paper's coauthors (@RTomMcCoy) responds and further discussion develops,neurocompositional computing;Microsoft,2
83,'1523026398176153600',33.74685683,0.307944884,240,81,159,12,44,14/02/1900 14:34,05/07/2022 19:46,"AI hitting a wall, seems to be Marcus' reply to LaCun's reply to Marcus' previous comment on DL hitting the wall (conv ID '1523026398176153600'); examples of Tesla cars failing (disagreement if the system was AI-powered)","AI hitting a wall, autonomous vehicles;",2
84,'1523371645561294848',34.75069021,0.194154479,81,68,13,6,41,10/02/1900 10:28,05/08/2022 18:37,LeCun agrees with Francois Chollet on the false idea that DL researchers are unaware (and thus not interested in solving) of DL limitations; you can be aware of DL limitations and still use it to build useful things; some point on the hype of DL leading to the AGI; LeCun replies that he refutes the idea that AI is around the corner; (I think that this is an indirect reply to Marcus point that DL researchers don't communicate about the limitations of their models),AI politics;limitations of DL;communication;AGI,1
85,'1524007897914617858',35.7599519,0.09690142,148,119,29,13,68,09/03/1900 05:33,05/10/2022 12:46,"the threa is on what is a symbol; Marcus replies pointing to the 2nd chapter of ""The Algebraic Mind""; multiple definitions provided, some come from CS, some from semiotics; Felix Hill shares the link to ""The Symbolic Species"" book, which he's done at least one in the previous tweets pointing to its relevance to AGI;","Symbol, AGI",2
86,'1525397036325019649',36.76848831,-1.031403112,257,226,31,13,147,26/05/1900 13:46,5/14/2022 8:46,it's a reaction piece ot OpenAI's Gato multi-modal AI system; auhor argues that scale is all one needs to reach AGI; marcus replies by linking to his tweet,Scaling DL;AGI;OpenAI;Gato,2
87,'1525560489216028677',37.77504786,0.106094039,177,160,17,14,94,04/04/1900 00:53,5/14/2022 19:35,LeCun on the fact that learning representations are more important than accurate inferences with shallow models; LeCun does not argue against the usefulness of inference which can/will become useful again when machines learn to be proactive rather than reactive,DL and learning representations;DL and inference,1
88,'1526388274348150784',38.7826655,-0.960528704,239,211,28,18,102,12/04/1900 13:49,5/17/2022 2:24,"it's a thread that DL can progress by scaling it up (which is yet to reach its ceiling), which could then lead to AGI; Some argue that human-level intelligence requires more than higher levels of abstraction",Scaling DL;AGI,2
89,'1528768392118751234',39.7890679,0.111225942,365,343,22,16,208,26/07/1900 14:44,5/23/2022 16:02,LeCun posts about multimodal human self-supervised learning; comparison to DL,(multimodal) human vs machine self-supervised learning,2
90,'1529209302883479552',40.79677609,0.054755333,84,70,14,34,19,07/02/1900 22:46,5/24/2022 21:14,a critical response to LeCun's tweet on gradient-based learning and reasoning; Domingos and Marcus point the limits of DL to handle propositional logic;,"propositional logic,",0
91,'1529623114400681984',41.80551199,-0.110817782,79,56,23,13,25,28/01/1900 04:16,5/26/2022 0:39,Marcus on GPT-3 lacking semantics (language understanding),GPT-3;semantics,0
92,'1530633328784658433',42.81176319,0.110449136,79,71,8,7,39,08/02/1900 14:57,5/28/2022 19:33,"in this thread @Plinz argues that AI might not need symbols and if it does then it will figure out how to create them; Marcus reacts by sharing Sinatra's ""the impossible dream""; there's general questioning of this statement too",Symbolic AI vs DL;hybrid models for AI;,2
93,'1531718922688290816',43.8186071,-0.789378917,126,126,0,13,59,29/02/1900 09:57,5/31/2022 19:27,"Marcus reacts to the tweet (that reacts to Elon Musk) that we will have AGI by 2029; there's disagreement with some of the Marcus claims, such as wondering if Marcus criteria is sufficient for AGI (more like necessary rather than sufficient) with which Marcus agrees","AGI, Musk",2
94,'1534082344096702464',44.82584367,0.103802942,201,160,41,14,62,03/03/1900 13:27,06/07/2022 07:58,"In this thread @ChrSzegedy argues that AI will have stronger or on par math formalisation and theorem proving capabilities as human matematicians by 2029; Marcus replied that while he's curious, but is less interested in the narrow AI (he betted $100)",AI and mathematics (formalisation and theory proving),1
95,'1538178998282248193',45.83371502,0.112706047,119,108,11,8,61,01/03/1900 12:32,6/18/2022 15:17,"Marcus argues that current AI systems are indifferent to truth; LLM are unaware of intention; there are questions about the criteria for establishing ""truth""; some note the issue is not with the models but their use","LLM, truth",2
96,'948811917593780225',46.84100007,0.113309956,340,152,188,24,53,27/02/1900 04:20,01/04/2018 07:02,tdietterich responds to Marcus' paper on the limits of DL for AGI,definition of intelligence;AGI,1
97,1284557072118550532',47.84936731,-0.106526552,86,85,1,23,32,08/02/1900 09:47,7/18/2020 18:34,"it's on GPT-3 and the hype around it; the discussion goes around GPT-3 being exciting, but not it does not mean it understand the language and, definitely, is not AGI",GPT-3;semantics,0
98,1409940043951742981',48.85777668,0.660749287,186,156,30,23,74,17/03/1900 11:48,6/29/2021 18:21,"LeCun reacts to Pinker; in general the thread is about DL and interpolation/extrapolation (curve fitting); critque goes that DL is not as impressive because it's an interpolation, but LeCun counterclaism that in high dimensional space eveything is extrapolation; LeCun again brings a point that some people (I assume he means Marcus) push their agendas",DL hitting a wall;AI politics,1
99,1411417104298004480',49.86246834,0.115900886,97,88,9,26,19,01/02/1900 04:51,07/03/2021 20:10,this thread is about the the lack of DL generalisability and poor performance of complex behaviour;,DL generalisability,1